##| results: markup
#| results: asis
options(scipen = 999)
# construct a GLM based on the binomial distribution
model <- glm(
high_rating ~ years_since + length + budget + votes + genre,
family = binomial(link = 'logit'),
data = df,method = "brglmFit"
)
# summary of the model
summary(model)
# Chunk 21
#| results: asis
stargazer(model,
type = "latex",  # PDF输出
title = "Regression Model Coefficients",
dep.var.labels = "High Rating (1 = Yes)",
covariate.labels = c(
"Years Since", "Length", "Budget", "Votes",
"Genre: Animation", "Genre: Comedy", "Genre: Documentary",
"Genre: Drama", "Genre: Romance", "Genre: Short"
),
report = "vcsp",           # 显示 coefficient, std. error, z value, p-value
digits = 3,
header = TRUE,             # 显示列名（Est. / SE / z / p）
no.space = TRUE,
omit.stat = c("f", "ser", "ll"))
# Chunk 22
#Check which samples have predicted probabilities exactly equal to 0 or 1
pred_probs <- predict(model, type = "response")
summary(pred_probs)
#Find and display the samples
df_extreme <- df[pred_probs %in% c(0, 1), ]
print(df_extreme)
# Chunk 23
#| results: markup
# establish models
null_model <- glm(high_rating ~ 1, family = binomial(), data = df)
# likelihood ratio test
anova_result <- anova(null_model, model, test = "Chisq")
print(anova_result)
# Chunk 24
#| results: markup
# Wald test锛圱ype III test锛?
wald_test <- Anova(model, type = "III", test = "Wald")
print(wald_test)
# Chunk 25
#| results: markup
df$predicted_prob <- predict(model, type = "response")
# Hosmer-Lemeshow test
hl_test <- hoslem.test(as.numeric(df$high_rating), df$predicted_prob, g = 10)
# outcome
print(hl_test)
# Chunk 26: fig-residual
#| echo: true
#| label: fig-residual
#| fig.cap: "Residuals vs Fitted"
#| fig-align: center
# residuals vs fitted
df_residuals<-data.frame(
Fitted=fitted(model),
Residuals=residuals(model)
)
ggplot(df_residuals,aes(x=Fitted,y=Residuals))+
geom_point(color="blue",alpha=0.6)+
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(x="Fitted Values",y="Residuals")
# Chunk 27: fig-QQ
#| label: fig-QQ
#| fig.cap: "Normal QQ plot"
#| fig-align: center
# Q-Q plot
qq_data<-data.frame(
sample=residuals(model),
theoretical=qqnorm(residuals(model))
)
# Chunk 28: fig-residuals
#| label: fig-residuals
#| fig.cap: "Q-Q Plot of Residuals"
#| fig-align: center
ggplot(qq_data,aes(sample=sample))+
stat_qq()+
stat_qq_line(color="red")+
labs(x="Theoretical Quantiles",
y="Sample Quantiles")
# Chunk 29: fig-ROC
#| label: fig-ROC
#| fig.cap: "ROC Curve"
#| fig-align: center
# prediction probability
pred_prob <- predict(model, type = "response")
# ROC curve
roc_curve <- roc(response = df$high_rating, predictor = pred_prob)
par(mfrow=c(1,1))
par(pty="s")
plot(roc_curve, col="blue",lwd=2)
auc_value <- auc(roc_curve)
cat("AUC =", auc_value, "\n")
# Chunk 30
# prediction class
pred_class <- ifelse(pred_prob > 0.5, 1, 0)
# precision
metrics <- confusionMatrix(as.factor(pred_class), df$high_rating)
print(metrics)
# Chunk 1: libraries
#| label: libraries
#Load the required R packages
library(stargazer)
library(naniar)
library(dplyr)
library(fastDummies)
library(lmtest)
library(car)
library(pROC)
library(brglm2)
library(ggplot2)
library(GGally)
library(gridExtra)
library(ResourceSelection)
library(caret)
# Chunk 2
#read the data
df <- read.csv("../Data/dataset09.csv", stringsAsFactors = FALSE)
str<-str(df)
summary<-summary(df)
print(summary)
# Chunk 3
mcar_test(df["length"])
# Chunk 4
df <- df %>% filter(!is.na(length))
# Chunk 5
# convert y
df$high_rating <- ifelse(df$rating > 7, 1, 0)
# Chunk 6
df$genre <- factor(df$genre)
df$high_rating <- factor(df$high_rating)
# Chunk 7
df$years_since <- 2025 - df$year
# Chunk 8
# Set up linear regression model 1
model_1 <- glm(high_rating ~ year + length + budget + votes + genre ,
data = df,
family = binomial)
# Durbin-Watson checking
dwtest(model_1)
# Chunk 9
# Check for duplicate values and abnormal patterns
dup<-sum(duplicated(df))
# Chunk 10
# Set up linear regression model 2
model_2 <- glm(high_rating ~ year + length + budget
+ votes + genre,
data = df, family = binomial)
# Calculate VIF
vif(model_2)
# Chunk 11
# Calculate standardized residuals
model_residuals <- rstandard(model_1)
# Identify outliers with an absolute value greater than 3
outliers <- which(abs(model_residuals) > 3)
print(outliers)
# Chunk 12
print(df[outliers, ])
# Chunk 13
# Calculate leverage values
leverage <- hatvalues(model_1)
# Calculate the leverage threshold
threshold <- 2 * (length(coef(model_1)) / nrow(df))
# Identify points with high leverage
high_leverage <- which(leverage > threshold)
# Chunk 14
lev<-df[high_leverage, c("year", "length", "budget", "votes", "high_rating")]
# Chunk 15
cooks_values <- cooks.distance(model_1)
influential_points <- which(cooks_values > 1)
print(intersect(high_leverage, influential_points))
# Chunk 16
write.csv(x = df,file = "Cleaned_data.csv",row.names = FALSE)
# Chunk 17
#| echo: true
#| fig.cap: "Distribution of IMDB Rating"
#| fig-align: center
ggplot(df, aes(x=rating))+
geom_histogram(binwidth=0.5,fill="blue",alpha=0.6)+
theme_minimal()
# Chunk 18: fig-indicators
#| label: fig-indicators
#| fig-cap: "key indicators influencing rating"
# Rating vs Budget
p1<-ggplot(df,aes(x=budget,y=rating))+
geom_point(alpha=0.5,color="blue")+
geom_smooth(method="lm",color="red")+
labs(title="Rating vs Budget",x="Budget",y="IMDB Rating")
# Rating vs Votes
p2<-ggplot(df,aes(x=votes,y=rating))+
geom_point(alpha=0.5,color="purple")+
geom_smooth(method="lm",color="red")+
labs(title="Rating vs Votes",x="Votes",y="IMDB Rating")
# Rating by Genre
p3<-df %>%
group_by(genre)%>%
summarise(avg_rating=mean(rating,na.rm=TRUE))%>%
ggplot(aes(x=reorder(genre,avg_rating),y=avg_rating,fill=genre))+
geom_bar(stat="identity",show.legend = FALSE)+
labs(title="Average Rating by Genre",x="Genre",y="Average Rating")
grid.arrange(p1,p2,p3,nrow=1)
# Chunk 19: fig-relationships
#| label: fig-relationships
#| fig-cap: "Relationships Between Movie Length, Budget, Votes, and Rating"
ggpairs(df,c("length", "budget", "votes","rating"))
# Chunk 20
#| echo: false
##| results: markup
#| results: asis
options(scipen = 999)
# construct a GLM based on the binomial distribution
model <- glm(
high_rating ~ years_since + length + budget + votes + genre,
family = binomial(link = 'logit'),
data = df,method = "brglmFit"
)
# summary of the model
summary(model)
# Chunk 21
#| results: asis
stargazer(model,
type = "latex",
title = "Regression Model Coefficients",
dep.var.labels = "High Rating (1 = Yes)",
covariate.labels = c(
"Years Since", "Length", "Budget", "Votes",
"Genre: Animation", "Genre: Comedy", "Genre: Documentary",
"Genre: Drama", "Genre: Romance", "Genre: Short"
),
omit.stat = c("f", "ser", "ll"),
digits = 3,
report = "vcsp",         # 显示估计值、标准误、z值和p值
single.row = TRUE,       # ✅ 所有值一行显示（关键！）
header = FALSE,          # ✅ 若你用 Quarto 会自动加 header，不加反而更兼容
column.labels = c("Estimate", "Std. Error", "z value", "p-value"),
no.space = TRUE)
# Chunk 22
#Check which samples have predicted probabilities exactly equal to 0 or 1
pred_probs <- predict(model, type = "response")
summary(pred_probs)
#Find and display the samples
df_extreme <- df[pred_probs %in% c(0, 1), ]
print(df_extreme)
# Chunk 23
#| results: markup
# establish models
null_model <- glm(high_rating ~ 1, family = binomial(), data = df)
# likelihood ratio test
anova_result <- anova(null_model, model, test = "Chisq")
print(anova_result)
# Chunk 24
#| results: markup
# Wald test锛圱ype III test锛?
wald_test <- Anova(model, type = "III", test = "Wald")
print(wald_test)
# Chunk 25
#| results: markup
df$predicted_prob <- predict(model, type = "response")
# Hosmer-Lemeshow test
hl_test <- hoslem.test(as.numeric(df$high_rating), df$predicted_prob, g = 10)
# outcome
print(hl_test)
# Chunk 26: fig-residual
#| echo: true
#| label: fig-residual
#| fig.cap: "Residuals vs Fitted"
#| fig-align: center
# residuals vs fitted
df_residuals<-data.frame(
Fitted=fitted(model),
Residuals=residuals(model)
)
ggplot(df_residuals,aes(x=Fitted,y=Residuals))+
geom_point(color="blue",alpha=0.6)+
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(x="Fitted Values",y="Residuals")
# Chunk 27: fig-QQ
#| label: fig-QQ
#| fig.cap: "Normal QQ plot"
#| fig-align: center
# Q-Q plot
qq_data<-data.frame(
sample=residuals(model),
theoretical=qqnorm(residuals(model))
)
# Chunk 28: fig-residuals
#| label: fig-residuals
#| fig.cap: "Q-Q Plot of Residuals"
#| fig-align: center
ggplot(qq_data,aes(sample=sample))+
stat_qq()+
stat_qq_line(color="red")+
labs(x="Theoretical Quantiles",
y="Sample Quantiles")
# Chunk 29: fig-ROC
#| label: fig-ROC
#| fig.cap: "ROC Curve"
#| fig-align: center
# prediction probability
pred_prob <- predict(model, type = "response")
# ROC curve
roc_curve <- roc(response = df$high_rating, predictor = pred_prob)
par(mfrow=c(1,1))
par(pty="s")
plot(roc_curve, col="blue",lwd=2)
auc_value <- auc(roc_curve)
cat("AUC =", auc_value, "\n")
# Chunk 30
# prediction class
pred_class <- ifelse(pred_prob > 0.5, 1, 0)
# precision
metrics <- confusionMatrix(as.factor(pred_class), df$high_rating)
print(metrics)
# Chunk 1: libraries
#| label: libraries
#Load the required R packages
library(naniar)
library(dplyr)
library(fastDummies)
library(lmtest)
library(car)
library(pROC)
library(brglm2)
library(ggplot2)
library(GGally)
library(gridExtra)
library(ResourceSelection)
library(caret)
# Chunk 2
#read the data
df <- read.csv("../Data/dataset09.csv", stringsAsFactors = FALSE)
str<-str(df)
summary<-summary(df)
print(summary)
# Chunk 3
mcar_test(df["length"])
# Chunk 4
df <- df %>% filter(!is.na(length))
# Chunk 5
# convert y
df$high_rating <- ifelse(df$rating > 7, 1, 0)
# Chunk 6
df$genre <- factor(df$genre)
df$high_rating <- factor(df$high_rating)
# Chunk 7
df$years_since <- 2025 - df$year
# Chunk 8
# Set up linear regression model 1
model_1 <- glm(high_rating ~ year + length + budget + votes + genre ,
data = df,
family = binomial)
# Durbin-Watson checking
dwtest(model_1)
# Chunk 9
# Check for duplicate values and abnormal patterns
dup<-sum(duplicated(df))
# Chunk 10
# Set up linear regression model 2
model_2 <- glm(high_rating ~ year + length + budget
+ votes + genre,
data = df, family = binomial)
# Calculate VIF
vif(model_2)
# Chunk 11
# Calculate standardized residuals
model_residuals <- rstandard(model_1)
# Identify outliers with an absolute value greater than 3
outliers <- which(abs(model_residuals) > 3)
print(outliers)
# Chunk 12
print(df[outliers, ])
# Chunk 13
# Calculate leverage values
leverage <- hatvalues(model_1)
# Calculate the leverage threshold
threshold <- 2 * (length(coef(model_1)) / nrow(df))
# Identify points with high leverage
high_leverage <- which(leverage > threshold)
# Chunk 14
lev<-df[high_leverage, c("year", "length", "budget", "votes", "high_rating")]
# Chunk 15
cooks_values <- cooks.distance(model_1)
influential_points <- which(cooks_values > 1)
print(intersect(high_leverage, influential_points))
# Chunk 16
write.csv(x = df,file = "Cleaned_data.csv",row.names = FALSE)
# Chunk 17
#| echo: true
#| fig.cap: "Distribution of IMDB Rating"
#| fig-align: center
ggplot(df, aes(x=rating))+
geom_histogram(binwidth=0.5,fill="blue",alpha=0.6)+
theme_minimal()
# Chunk 18: fig-indicators
#| label: fig-indicators
#| fig-cap: "key indicators influencing rating"
# Rating vs Budget
p1<-ggplot(df,aes(x=budget,y=rating))+
geom_point(alpha=0.5,color="blue")+
geom_smooth(method="lm",color="red")+
labs(title="Rating vs Budget",x="Budget",y="IMDB Rating")
# Rating vs Votes
p2<-ggplot(df,aes(x=votes,y=rating))+
geom_point(alpha=0.5,color="purple")+
geom_smooth(method="lm",color="red")+
labs(title="Rating vs Votes",x="Votes",y="IMDB Rating")
# Rating by Genre
p3<-df %>%
group_by(genre)%>%
summarise(avg_rating=mean(rating,na.rm=TRUE))%>%
ggplot(aes(x=reorder(genre,avg_rating),y=avg_rating,fill=genre))+
geom_bar(stat="identity",show.legend = FALSE)+
labs(title="Average Rating by Genre",x="Genre",y="Average Rating")
grid.arrange(p1,p2,p3,nrow=1)
# Chunk 19: fig-relationships
#| label: fig-relationships
#| fig-cap: "Relationships Between Movie Length, Budget, Votes, and Rating"
ggpairs(df,c("length", "budget", "votes","rating"))
# Chunk 20
#| echo: false
#| results: markup
options(scipen = 999)
# construct a GLM based on the binomial distribution
model <- glm(
high_rating ~ years_since + length + budget + votes + genre,
family = binomial(link = 'logit'),
data = df,method = "brglmFit"
)
# summary of the model
summary(model)
# Chunk 21
#Check which samples have predicted probabilities exactly equal to 0 or 1
pred_probs <- predict(model, type = "response")
summary(pred_probs)
#Find and display the samples
df_extreme <- df[pred_probs %in% c(0, 1), ]
print(df_extreme)
# Chunk 22
#| results: markup
# establish models
null_model <- glm(high_rating ~ 1, family = binomial(), data = df)
# likelihood ratio test
anova_result <- anova(null_model, model, test = "Chisq")
print(anova_result)
# Chunk 23
#| results: markup
# Wald test锛圱ype III test锛?
wald_test <- Anova(model, type = "III", test = "Wald")
print(wald_test)
# Chunk 24
#| results: markup
df$predicted_prob <- predict(model, type = "response")
# Hosmer-Lemeshow test
hl_test <- hoslem.test(as.numeric(df$high_rating), df$predicted_prob, g = 10)
# outcome
print(hl_test)
# Chunk 25: fig-residual
#| echo: true
#| label: fig-residual
#| fig.cap: "Residuals vs Fitted"
#| fig-align: center
# residuals vs fitted
df_residuals<-data.frame(
Fitted=fitted(model),
Residuals=residuals(model)
)
ggplot(df_residuals,aes(x=Fitted,y=Residuals))+
geom_point(color="blue",alpha=0.6)+
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(x="Fitted Values",y="Residuals")
# Chunk 26: fig-QQ
#| label: fig-QQ
#| fig.cap: "Normal QQ plot"
#| fig-align: center
# Q-Q plot
qq_data<-data.frame(
sample=residuals(model),
theoretical=qqnorm(residuals(model))
)
# Chunk 27: fig-residuals
#| label: fig-residuals
#| fig.cap: "Q-Q Plot of Residuals"
#| fig-align: center
ggplot(qq_data,aes(sample=sample))+
stat_qq()+
stat_qq_line(color="red")+
labs(x="Theoretical Quantiles",
y="Sample Quantiles")
# Chunk 28: fig-ROC
#| label: fig-ROC
#| fig.cap: "ROC Curve"
#| fig-align: center
# prediction probability
pred_prob <- predict(model, type = "response")
# ROC curve
roc_curve <- roc(response = df$high_rating, predictor = pred_prob)
par(mfrow=c(1,1))
par(pty="s")
plot(roc_curve, col="blue",lwd=2)
auc_value <- auc(roc_curve)
cat("AUC =", auc_value, "\n")
# Chunk 29
# prediction class
pred_class <- ifelse(pred_prob > 0.5, 1, 0)
# precision
metrics <- confusionMatrix(as.factor(pred_class), df$high_rating)
print(metrics)
