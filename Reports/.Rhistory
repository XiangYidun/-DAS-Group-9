# Chunk 1
library(naniar)
library(dplyr)
library(fastDummies)
library(lmtest)
library(car)
library(pROC)
library(brglm2)
library(ggplot2)
library(GGally)
library(gridExtra)
library(ResourceSelection)
library(caret)
# Chunk 2
df <- read.csv("C:/Users/2936935l/Downloads/dataset09.csv", stringsAsFactors = FALSE)
# Chunk 1
library(naniar)
library(dplyr)
library(fastDummies)
library(lmtest)
library(car)
library(pROC)
library(brglm2)
library(ggplot2)
library(GGally)
library(gridExtra)
library(ResourceSelection)
library(caret)
df <- read.csv("../Data_Cleaning/dataset09.csv", stringsAsFactors = FALSE)
df <- read.csv("../Data_Cleaning/dataset09.csv", stringsAsFactors = FALSE)
df <- read.csv("../Data_Cleaning/dataset09.csv", stringsAsFactors = FALSE)
df <- read.csv("../Data_Cleaning/dataset09.csv", stringsAsFactors = FALSE)
df <- read.csv("../Data/dataset09.csv", stringsAsFactors = FALSE)
df <- read.csv("../Data/dataset09.csv", stringsAsFactors = FALSE)
# Chunk 1
library(naniar)
library(dplyr)
library(fastDummies)
library(lmtest)
library(car)
library(pROC)
library(brglm2)
library(ggplot2)
library(GGally)
library(gridExtra)
library(ResourceSelection)
library(caret)
# Chunk 2
df <- read.csv("../Data/dataset09.csv", stringsAsFactors = FALSE)
# Chunk 3
str<-str(df)
summary<-summary(df)
# Chunk 4
ggplot(df, aes(x=rating))+
geom_histogram(binwidth=0.5,fill="blue",alpha=0.6)+
labs(title="Distribution of IMDB Rating")+
theme_minimal()
# Chunk 5
# Rating vs Budget
p1<-ggplot(df,aes(x=budget,y=rating))+
geom_point(alpha=0.5,color="blue")+
geom_smooth(method="lm",color="red")+
labs(title="Rating vs Budget",x="Budget",y="IMDB Rating")
# Rating vs Votes
p2<-ggplot(df,aes(x=votes,y=rating))+
geom_point(alpha=0.5,color="purple")+
geom_smooth(method="lm",color="red")+
labs(title="Rating vs Votes",x="Votes",y="IMDB Rating")
# Rating by Genre
p3<-df %>%
group_by(genre)%>%
summarise(avg_rating=mean(rating,na.rm=TRUE))%>%
ggplot(aes(x=reorder(genre,avg_rating),y=avg_rating,fill=genre))+
geom_bar(stat="identity",show.legend = FALSE)+
labs(title="Average Rating by Genre",x="Genre",y="Average Rating")
grid.arrange(p1,p2,p3,nrow=1)
# Chunk 6
ggpairs(df,c("length", "budget", "votes","rating"))
# Chunk 7
mcar_test(df["length"])
# Chunk 8
df <- df %>% filter(!is.na(length))
# Chunk 9
# convert y
df$high_rating <- ifelse(df$rating > 7, 1, 0)
# Chunk 10
df$genre <- factor(df$genre)
df$high_rating <- factor(df$high_rating)
# Chunk 11
df$years_since <- 2025 - df$year
# Chunk 12
# Set up linear regression model 1
model_1 <- glm(high_rating ~ year + length + budget + votes + genre ,data = df, family = binomial)
# Durbin-Watson checking
dwtest(model_1)
# Chunk 13
# Check for duplicate values and abnormal patterns
dup<-sum(duplicated(df))
# Chunk 14
# Set up linear regression model 2
model_2 <- glm(high_rating ~ year + length + budget
+ votes + genre,
data = df, family = binomial)
# Calculate VIF
vif(model_2)
# Chunk 15
# Calculate standardized residuals
model_residuals <- rstandard(model_1)
# Identify outliers with an absolute value greater than 3
outliers <- which(abs(model_residuals) > 3)
print(outliers)
# Chunk 16
print(df[outliers, ])
# Chunk 17
# Calculate leverage values
leverage <- hatvalues(model_1)
# Calculate the leverage threshold
threshold <- 2 * (length(coef(model_1)) / nrow(df))
# Identify points with high leverage
high_leverage <- which(leverage > threshold)
# Chunk 18
lev<-df[high_leverage, c("year", "length", "budget", "votes", "high_rating")]
# Chunk 19
cooks_values <- cooks.distance(model_1)
influential_points <- which(cooks_values > 1)
print(intersect(high_leverage, influential_points))
# Chunk 20
write.csv(x = df,file = "Cleaned_data.csv",row.names = FALSE)
# Chunk 21
#| echo: false
options(scipen = 999)
# construct a GLM based on the binomial distribution
model <- glm(
high_rating ~ years_since + length + budget + votes + genre,
family = binomial(link = 'logit'),
data = df,method = "brglmFit"
)
# summary of the model
summary(model)
# Chunk 22
#Check which samples have predicted probabilities exactly equal to 0 or 1
pred_probs <- predict(model, type = "response")
summary(pred_probs)
#Find and display the samples
df_extreme <- df[pred_probs %in% c(0, 1), ]
print(df_extreme)
# Chunk 23
# establish models
null_model <- glm(high_rating ~ 1, family = binomial(), data = df)
# likelihood ratio test
anova_result <- anova(null_model, model, test = "Chisq")
print(anova_result)
# Chunk 24
# Wald test（Type III test）
wald_test <- Anova(model, type = "III", test = "Wald")
print(wald_test)
# Chunk 25
df$predicted_prob <- predict(model, type = "response")
# Hosmer-Lemeshow test
hl_test <- hoslem.test(as.numeric(df$high_rating), df$predicted_prob, g = 10)
# outcome
print(hl_test)
# Chunk 26
# residuals vs fitted
df_residuals<-data.frame(
Fitted=fitted(model),
Residuals=residuals(model)
)
ggplot(df_residuals,aes(x=Fitted,y=Residuals))+
geom_point(color="blue",alpha=0.6)+
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title="Residuals vs Fitted",
x="Fitted Values",
y="Residuals")
# Chunk 27
# Q-Q plot
qq_data<-data.frame(
sample=residuals(model),
theoretical=qqnorm(residuals(model))
)
ggplot(qq_data,aes(sample=sample))+
stat_qq()+
stat_qq_line(color="red")+
labs(title="Q-Q Plot of Residuals",
x="Theoretical Quantiles",
y="Sample Quantiles")
# Chunk 28
# prediction probability
pred_prob <- predict(model, type = "response")
# ROC curve
roc_curve <- roc(response = df$high_rating, predictor = pred_prob)
par(mfrow=c(1,1))
par(pty="s")
plot(roc_curve,main="ROC Curve", col="blue",lwd=2)
auc_value <- auc(roc_curve)
cat("AUC =", auc_value, "\n")
# Chunk 29
# prediction class
pred_class <- ifelse(pred_prob > 0.5, 1, 0)
# precision
metrics <- confusionMatrix(as.factor(pred_class), df$high_rating)
print(metrics)
# residuals vs fitted
df_residuals<-data.frame(
Fitted=fitted(model),
Residuals=residuals(model)
)
ggplot(df_residuals,aes(x=Fitted,y=Residuals))+
geom_point(color="blue",alpha=0.6)+
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title="Residuals vs Fitted",
x="Fitted Values",
y="Residuals")
# Chunk 1
library(naniar)
library(dplyr)
library(fastDummies)
library(lmtest)
library(car)
library(pROC)
library(brglm2)
library(ggplot2)
library(GGally)
library(gridExtra)
library(ResourceSelection)
library(caret)
# Chunk 2
df <- read.csv("../Data/dataset09.csv", stringsAsFactors = FALSE)
# Chunk 3
ggplot(df, aes(x=rating))+
geom_histogram(binwidth=0.5,fill="blue",alpha=0.6)+
labs(title="Distribution of IMDB Rating")+
theme_minimal()
# Chunk 4
# Rating vs Budget
p1<-ggplot(df,aes(x=budget,y=rating))+
geom_point(alpha=0.5,color="blue")+
geom_smooth(method="lm",color="red")+
labs(title="Rating vs Budget",x="Budget",y="IMDB Rating")
# Rating vs Votes
p2<-ggplot(df,aes(x=votes,y=rating))+
geom_point(alpha=0.5,color="purple")+
geom_smooth(method="lm",color="red")+
labs(title="Rating vs Votes",x="Votes",y="IMDB Rating")
# Rating by Genre
p3<-df %>%
group_by(genre)%>%
summarise(avg_rating=mean(rating,na.rm=TRUE))%>%
ggplot(aes(x=reorder(genre,avg_rating),y=avg_rating,fill=genre))+
geom_bar(stat="identity",show.legend = FALSE)+
labs(title="Average Rating by Genre",x="Genre",y="Average Rating")
grid.arrange(p1,p2,p3,nrow=1)
# Chunk 5
ggpairs(df,c("length", "budget", "votes","rating"))
# Chunk 6
str<-str(df)
summary<-summary(df)
# Chunk 7
mcar_test(df["length"])
# Chunk 8
df <- df %>% filter(!is.na(length))
# Chunk 9
# convert y
df$high_rating <- ifelse(df$rating > 7, 1, 0)
# Chunk 10
df$genre <- factor(df$genre)
df$high_rating <- factor(df$high_rating)
# Chunk 11
df$years_since <- 2025 - df$year
# Chunk 12
# Set up linear regression model 1
model_1 <- glm(high_rating ~ year + length + budget + votes + genre ,data = df, family = binomial)
# Durbin-Watson checking
dwtest(model_1)
# Chunk 13
# Check for duplicate values and abnormal patterns
dup<-sum(duplicated(df))
# Chunk 14
# Set up linear regression model 2
model_2 <- glm(high_rating ~ year + length + budget
+ votes + genre,
data = df, family = binomial)
# Calculate VIF
vif(model_2)
# Chunk 15
# Calculate standardized residuals
model_residuals <- rstandard(model_1)
# Identify outliers with an absolute value greater than 3
outliers <- which(abs(model_residuals) > 3)
print(outliers)
# Chunk 16
print(df[outliers, ])
# Chunk 17
# Calculate leverage values
leverage <- hatvalues(model_1)
# Calculate the leverage threshold
threshold <- 2 * (length(coef(model_1)) / nrow(df))
# Identify points with high leverage
high_leverage <- which(leverage > threshold)
# Chunk 18
lev<-df[high_leverage, c("year", "length", "budget", "votes", "high_rating")]
# Chunk 19
cooks_values <- cooks.distance(model_1)
influential_points <- which(cooks_values > 1)
print(intersect(high_leverage, influential_points))
# Chunk 20
write.csv(x = df,file = "Cleaned_data.csv",row.names = FALSE)
# Chunk 21
#| echo: false
options(scipen = 999)
# construct a GLM based on the binomial distribution
model <- glm(
high_rating ~ years_since + length + budget + votes + genre,
family = binomial(link = 'logit'),
data = df,method = "brglmFit"
)
# summary of the model
summary(model)
# Chunk 22
#Check which samples have predicted probabilities exactly equal to 0 or 1
pred_probs <- predict(model, type = "response")
summary(pred_probs)
#Find and display the samples
df_extreme <- df[pred_probs %in% c(0, 1), ]
print(df_extreme)
# Chunk 23
# establish models
null_model <- glm(high_rating ~ 1, family = binomial(), data = df)
# likelihood ratio test
anova_result <- anova(null_model, model, test = "Chisq")
print(anova_result)
# Chunk 24
# Wald test（Type III test）
wald_test <- Anova(model, type = "III", test = "Wald")
print(wald_test)
# Chunk 25
df$predicted_prob <- predict(model, type = "response")
# Hosmer-Lemeshow test
hl_test <- hoslem.test(as.numeric(df$high_rating), df$predicted_prob, g = 10)
# outcome
print(hl_test)
# Chunk 26
# residuals vs fitted
df_residuals<-data.frame(
Fitted=fitted(model),
Residuals=residuals(model)
)
ggplot(df_residuals,aes(x=Fitted,y=Residuals))+
geom_point(color="blue",alpha=0.6)+
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title="Residuals vs Fitted",
x="Fitted Values",
y="Residuals")
# Chunk 27
# Q-Q plot
qq_data<-data.frame(
sample=residuals(model),
theoretical=qqnorm(residuals(model))
)
ggplot(qq_data,aes(sample=sample))+
stat_qq()+
stat_qq_line(color="red")+
labs(title="Q-Q Plot of Residuals",
x="Theoretical Quantiles",
y="Sample Quantiles")
# Chunk 28
# prediction probability
pred_prob <- predict(model, type = "response")
# ROC curve
roc_curve <- roc(response = df$high_rating, predictor = pred_prob)
par(mfrow=c(1,1))
par(pty="s")
plot(roc_curve,main="ROC Curve", col="blue",lwd=2)
auc_value <- auc(roc_curve)
cat("AUC =", auc_value, "\n")
# Chunk 29
# prediction class
pred_class <- ifelse(pred_prob > 0.5, 1, 0)
# precision
metrics <- confusionMatrix(as.factor(pred_class), df$high_rating)
print(metrics)
