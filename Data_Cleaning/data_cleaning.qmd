---
title: "Data-Cleaning"
author: "Diwen Jiang"
format: pdf
editor: visual
number-sections: true
---

To begin, we need to import the necessary packages in R.

```{r}
library(naniar)
library(dplyr)
library(fastDummies)
library(lmtest)
library(car)
```

# Data Cleaning

Based on the research question: "Which properties of films influence whether they are rated by IMDB as greater than 7 or not?", we can conduct a preliminary analysis. This represents a binary classification problem, where the response variable is dichotomous. Therefore, we anticipate that logistic regression may be a suitable model for this analysis.

## Import Data Set

First, we need to read the raw dataset.

Read the dataset file dataset09.csv from the Data branch.

```{r}
df <- read.csv("dataset09.csv", stringsAsFactors = FALSE)
```

Next step, conducting a preliminary examination of the dataset.

```{r}
str(df)
summary(df)
```

At this stage, we can clearly observe that the **length** variable contains some missing values. No other obvious anomalies are present in the dataset; however, further discussion and analysis are needed.

### Handle missing values

Before proceeding with the discussion, we can first handle the missing values.The first step is to check whether the missing values are Missing Completely at Random (MCAR).

```{r}
mcar_test(df["length"])
```

p value is equal to 1 (far greater than 0.05), meaning there is no systematic missing pattern. statistic is equal to 0 indicates that the MCAR assumption perfectly matches the missing data pattern. df is equal to 0 may suggest that the dataset has few variables or a simple missing pattern, implying that the missing data is completely random (MCAR) and can be directly removed.

```{r}
df <- df %>% filter(!is.na(length))
```

## Further Discussing

After deleting the missing values, we need to further discuss whether the data meets all the assumptions of logistic regression.

### The dependent variable must be categorical, with at least one independent variable. Independent variables can be either continuous or categorical.

We can see that the target response variable should be whether films are rated by IMDb as greater than 7 or not.Therefore, the response variable needs to be transformed.

```{r}
df$high_rating <- ifelse(df$rating > 7, 1, 0)
```

Additionally, the "genre" variable is a multinomial categorical variable, so it needs to be converted into dummy variables.

```{r}
df_coded <- dummy_cols(df, select_columns = "genre",
                         remove_first_dummy = FALSE,
                         remove_selected_columns = TRUE)
print(df_coded)
```

Now, all variables meet the requirements of this assumption.

### Each observation must be independent. The categories of categorical variables (including both the dependent and independent variables) must be exhaustive and mutually exclusive.

#### Check whether the observations are independent.

The "year" variable may introduce dependency in the data, so we use the Durbin-Watson test to check for autocorrelation:

```{r}

# Set up linear regression model 1
model_1 <- glm(high_rating ~ year + length + budget 
             + votes + genre_Action + genre_Animation + genre_Short 
             + genre_Documentary + genre_Romance + genre_Comedy + genre_Drama, 
             data = df_coded, family = binomial)

# Durbin-Watson checking
dwtest(model_1)

```

p-value is equal to 0.6249, which is far greater than 0.05, indicating that the independence assumption is satisfied.

Check for duplicate values and abnormal patterns:

```{r}
sum(duplicated(df_coded))
```

There are no duplicate values in the dataset.

#### Check whether categorical variables are mutually exclusive.

```{r}
# Check whether any films belong to multiple genres simultaneously.
sum(rowSums(df_coded[, c("genre_Action" , "genre_Animation" , "genre_Short" 
             , "genre_Documentary" , "genre_Romance" , 
             "genre_Comedy" , "genre_Drama")]) > 1)

```

This result Indicates that no films are classified into multiple genres simultaneously.

### The minimum sample size requirement

### is 15 times the number of independent variables.

The cleaned dataset contains 2,874 entries, which clearly supports this assumption.

### There should be no multicollinearity among independent variables.

VIF calculates the correlation between an independent variable and other independent variables.

```{r}
# Set up linear regression model 2
model_2 <- glm(high_rating ~ year + length + budget 
             + votes + genre_Action + genre_Animation + genre_Short 
             + genre_Documentary + genre_Romance + genre_Comedy, 
             data = df_coded, family = binomial)

# Calculate VIF
vif(model_2)
```

All VIF less than 5, indicating that there is no severe multicollinearity.

### There should be no significant outliers, leverage points, or influential points in the data.

Check for outliers:

```{r}
# Calculate standardized residuals
model_residuals <- rstandard(model_1)

# Identify outliers with an absolute value greater than 3
outliers <- which(abs(model_residuals) > 3)

print(outliers)
```

We found that rows 917, 1870, and 2388 may contain outliers.Now, print these three rows for inspection.

```{r}
print(df_coded[outliers, ])
```

After inspection, these three values are considered normal data and will be retained for now.

Check for leverage points:

```{r}
# Calculate leverage values
leverage <- hatvalues(model_1)

# Calculate the leverage threshold
threshold <- 2 * (length(coef(model_1)) / nrow(df_coded))

# Identify points with high leverage
high_leverage <- which(leverage > threshold)

print(high_leverage)

```

A large number of leverage values have been detected, requiring further investigation.

```{r}
print(df_coded[high_leverage, c("year", "length", "budget", "votes", "high_rating")])
```

Combine Cook's Distance to check which points are truly influential.

```{r}
cooks_values <- cooks.distance(model_1)
influential_points <- which(cooks_values > 1)

print(intersect(high_leverage, influential_points))

```

## Output

```{r}
write.csv(x = df_coded,file = "Cleaned_data.csv")
```


No observation is both a high leverage point and a \*\*highly influential point. Since the leverage points and Cook's Distance tests collectively suggest retaining the previously identified outliers, they will be kept. Now, the data supports this assumption.

The assumption that continuous independent variables have a linear relationship with the logit-transformed dependent variable will be further explored in later sections. This concludes the data preprocessing phase.
