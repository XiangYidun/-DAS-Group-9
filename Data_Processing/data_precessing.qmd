---
title: "Data-Cleaning"
author: "Diwen Jiang, Wei Xiang"
format: pdf
editor: visual
number-sections: true
---

To begin, we need to import the necessary packages in R.

```{r}
library(naniar)
library(dplyr)
library(fastDummies)
library(lmtest)
library(car)
library(pROC)
library(brglm2)
```

# Data Cleaning

Based on the research question: "Which properties of films influence whether they are rated by IMDB as greater than 7 or not?", we can conduct a preliminary analysis. This represents a binary classification problem, where the response variable is dichotomous. Therefore, we anticipate that logistic regression may be a suitable model for this analysis.

## Import Data Set

First, we need to read the raw dataset.

Read the dataset file dataset09.csv from the Data branch.

```{r}
df <- read.csv("dataset09.csv", stringsAsFactors = FALSE)
```

Next step, conducting a preliminary examination of the dataset.

```{r}
str(df)
summary(df)
```

At this stage, we can clearly observe that the **length** variable contains some missing values. No other obvious anomalies are present in the dataset; however, further discussion and analysis are needed.

### Handle missing values

Before proceeding with the discussion, we can first handle the missing values.The first step is to check whether the missing values are Missing Completely at Random (MCAR).

```{r}
mcar_test(df["length"])
```

p value is equal to 1 (far greater than 0.05), meaning there is no systematic missing pattern. statistic is equal to 0 indicates that the MCAR assumption perfectly matches the missing data pattern. df is equal to 0 may suggest that the dataset has few variables or a simple missing pattern, implying that the missing data is completely random (MCAR) and can be directly removed.

```{r}
df <- df %>% filter(!is.na(length))
```

## Further Discussing

After deleting the missing values, we need to further discuss whether the data meets all the assumptions of logistic regression.

### The dependent variable must be categorical, with at least one independent variable. Independent variables can be either continuous or categorical.

We can see that the target response variable should be whether films are rated by IMDb as greater than 7 or not.Therefore, the response variable needs to be transformed.

```{r}
df$high_rating <- ifelse(df$rating > 7, 1, 0)
```

Additionally, the "genre" variable is a multinomial categorical variable, so it needs to be converted into to a factor type.

```{r}
df$genre <- factor(df$genre)
df$high_rating <- factor(df$high_rating)
```

Now, all variables meet the requirements of this assumption.

#### Convert the year column to year_since

```{r}
df$years_since <- 2025 - df$year
```

### Each observation must be independent. The categories of categorical variables (including both the dependent and independent variables) must be exhaustive and mutually exclusive.

#### Check whether the observations are independent.

The "year" variable may introduce dependency in the data, so we use the Durbin-Watson test to check for autocorrelation:

```{r}

# Set up linear regression model 1
model_1 <- glm(high_rating ~ year + length + budget + votes + genre ,data = df, family = binomial)

# Durbin-Watson checking
dwtest(model_1)

```

p-value is equal to 0.9514, which is far greater than 0.05, indicating that the independence assumption is satisfied.

Check for duplicate values and abnormal patterns:

```{r}
sum(duplicated(df))
```

There are no duplicate values in the dataset.This result Indicates that no films are classified into multiple genres simultaneously.

### The minimum sample size requirement

### is 15 times the number of independent variables.

The cleaned dataset contains 2,874 entries, which clearly supports this assumption.

### There should be no multicollinearity among independent variables.

VIF calculates the correlation between an independent variable and other independent variables.

```{r}
# Set up linear regression model 2
model_2 <- glm(high_rating ~ year + length + budget 
             + votes + genre, 
             data = df, family = binomial)

# Calculate VIF
vif(model_2)
```

All VIF less than 5, indicating that there is no severe multicollinearity.

### There should be no significant outliers, leverage points, or influential points in the data.

Check for outliers:

```{r}
# Calculate standardized residuals
model_residuals <- rstandard(model_1)

# Identify outliers with an absolute value greater than 3
outliers <- which(abs(model_residuals) > 3)

print(outliers)
```

We found that rows 917, 1870, and 2388 may contain outliers .Now, print these three rows for inspection.

```{r}
print(df[outliers, ])
```

After inspection, these three values are considered normal data and will be retained for now.

Check for leverage points:

```{r}
# Calculate leverage values
leverage <- hatvalues(model_1)

# Calculate the leverage threshold
threshold <- 2 * (length(coef(model_1)) / nrow(df))

# Identify points with high leverage
high_leverage <- which(leverage > threshold)

print(high_leverage)

```

A large number of leverage values have been detected, requiring further investigation.

```{r}
print(df[high_leverage, c("year", "length", "budget", "votes", "high_rating")])
```

Combine Cook's Distance to check which points are truly influential.

```{r}
cooks_values <- cooks.distance(model_1)
influential_points <- which(cooks_values > 1)

print(intersect(high_leverage, influential_points))

```

## Output

```{r}
write.csv(x = df,file = "Cleaned_data.csv",row.names = FALSE)
```

No observation is both a high leverage point and a \*\*highly influential point. Since the leverage points and Cook's Distance tests collectively suggest retaining the previously identified outliers, they will be kept. Now, the data supports this assumption.

The assumption that continuous independent variables have a linear relationship with the logit-transformed dependent variable will be further explored in later sections. This concludes the data preprocessing phase.

## fitting model

```{r}
#| echo: false

options(scipen = 999)

# 建立二项分布广义线性模型
model <- glm(
  high_rating ~ years_since + length + budget + votes + genre,
  family = binomial(link = 'logit'),
  data = df,method = "brglmFit"
)

# 输出模型摘要
summary(model)
```

It is observed that in some cases the model produces predicted probabilities of exactly 0 or 1. This may be due to variables such as budget or votes having a very large range of values, which affects numerical computations and causes issues like quasi-complete separation in logistic regression.

```{r}
#Check which samples have predicted probabilities exactly equal to 0 or 1
pred_probs <- predict(model, type = "response")
summary(pred_probs)
#Find and display the samples
df_extreme <- df[pred_probs %in% c(0, 1), ]
print(df_extreme)
```

However, "df_extreme" is empty, indicating that there are no samples with predicted probabilities exactly equal to 0 or 1 — they are merely close to 0 or 1. Therefore, no adjustment is necessary.

## **Likelihood Ratio Test**

```{r}
# 构建基准模型和完整模型
null_model <- glm(high_rating ~ 1, family = binomial(), data = df)


# 执行似然比检验
anova_result <- anova(null_model, model, test = "Chisq")
print(anova_result)
```

**p \< 0.05**，说明完整模型显著优于基准模型。

## **Wald Test**

```{r}

# Wald卡方检验（Type III检验）
wald_test <- Anova(model, type = "III", test = "Wald")
print(wald_test)
```

**p \< 0.05**所有变量都显著

## **Hosmer-Lemeshow test**

```{r}
library(ResourceSelection)
df$predicted_prob <- predict(model, type = "response")

# 进行 Hosmer-Lemeshow 检验（分10组）
hl_test <- hoslem.test(as.numeric(df$high_rating), df$predicted_prob, g = 10)

# 输出检验结果
print(hl_test)
```

**p \> 0.05**，说明模型拟合良好

## residual analysis

```{r}
library(ggplot2)
deviance_resid <- residuals(model, type = "deviance")
# 使用 ggplot2 绘制残差图
ggplot(df, aes(x = predicted_prob, y = deviance_resid)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residual Plot", x = "Predicted Probability", y = "Deviance Residuals") +
  theme_minimal()
```

```{r}
# 1. 设置图形布局（1行2列）
par(mfrow = c(1, 2))  # 一行两列，显示两张图

# 2. 绘制残差vs拟合值图
plot(model, which = 1, main = "Residuals vs Fitted")

# 3. 绘制QQ图
plot(model, which = 2, main = "Q-Q Plot")
```

## ROC curve and AUC value

```{r}

# 预测概率
pred_prob <- predict(model, type = "response")

# 计算ROC曲线
roc_curve <- roc(response = df$high_rating, predictor = pred_prob)
plot(roc_curve)
auc_value <- auc(roc_curve)
cat("AUC =", auc_value, "\n")
```

```{r}
# 生成预测类别
pred_class <- ifelse(pred_prob > 0.5, 1, 0)

# 计算准确率、召回率等
library(caret)
metrics <- confusionMatrix(as.factor(pred_class), df$high_rating)
print(metrics)
```
